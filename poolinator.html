<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>An Nguyen</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/assets/css/main.css" />
		<noscript><link rel="stylesheet" href="/assets/css/noscript.css" /></noscript>
		<style>
			span.image.right {
				float: right; /* Aligns the image to the right */
				margin-left: 1em; /* Adds spacing between the image and text */
				margin-bottom: 1em; /* Adds space below the image */
				max-width: 40%; /* Limits the image width to prevent it from dominating */
			}

			ul {
				clear: both; /* Ensures the list clears the floated image */
				margin-top: 1em; /* Adds spacing above the list */
			}
			.block-diagram {
				display: block;
				margin: 1em auto; /* Centers the image horizontally */
			}
			.left-aligned-image {
				display: block; /* Ensures it behaves as a block element */
				margin-top: 1em; /* Adds space above the image */
				max-width: 100%; /* Ensures the image does not exceed the width of its container */
				height: auto;    /* Maintains the aspect ratio of the image */
				display: block;  /* Removes unwanted inline spacing around the image */
			}
		</style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">An Nguyen</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="/index.html">Projects</a></li>
							<li class="active"><a href="poolinator.html">Poolinator</a></li>
							<li><a href="/about.html">About Me</a></li>
							<li><a href="/resume.html">Resume</a></li>
						</ul>
						<ul class="icons">
							<li>
								<a href="mailto:annguyen2025@u.northwestern.edu" class="icon solid fa-envelope">
									<span class="label">Email</span>
								</a>
							</li>
							<li>
								<a href="https://www.linkedin.com/in/an-n-b2947b195/" class="icon brands fa-linkedin" target="_blank" rel="noopener noreferrer">
									<span class="label">LinkedIn</span>
								</a>
							</li>
							<li>
								<a href="https://github.com/annguyen9461" class="icon brands fa-github" target="_blank" rel="noopener noreferrer">
									<span class="label">GitHub</span>
								</a>
							</li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">December 14, 2024</span>
									<h1>Pool-inator</h1>
								</header>
								<div class="image main"><img src="images/pool.gif" alt="Franka playing pool" /></div>
								<h2>Overview</h2>
								<p>The Franka Emika Panda 7-DoF arm plays a game of pool by identifying balls and hitting them into pockets.</p>
								<p><u>Team Members</u>:  An Nguyen, Caroline Terryn, Catherine Maglione, Joseph Blom, Logan Boswell<br>
								<u>Github</u>: coming soon!
								</p>
								<h2>Components</h2>
								<div class="equipment-list">
									<span class="image right">
										<img src="images/pool-equipment-list.png" alt="poolinator equipment list" />
									</span>
										<li>Franka Emika Panda robot</li>
										<li>Intel RealSense Depth Camera D435i</li>
										<li>Pool table, balls, and custom cue</li>
								</div>
								<br>
								<br>
								<br>
								<br>
								<br>
								<h2>Project Sequence
								</h2>
								<ul>
									<li>The game begins with the robot arm moving to the side to avoid obscuring the table.
									</li>
									<li>The camera detects two AprilTags to determine the transformations of the table, the cue, the end-effector, and the camera, all relative to the base of the Franka.
									</li>
									<li>The locations of the cue ball (red) and other balls (blue) are updated, and these coordinates are fed into the pool algorithm.
									</li>
									<li>The robot selects a striking position that enables it to hit the cue ball and pocket another ball.
									</li>
									<li>After striking, the robot arm moves to the side again to update the new positions of the balls.
									</li>
									<li>The game continues until all the blue balls are pocketed.
									</li>
									<li>Then the robot hits the red ball into a pocket to end the game.
									</li>
								</ul>
								<h2>Nodes and Python Modules
								</h2>
								<p>The project is divided into several subsystems:
								</p>
								<dl>
									<dt>AprilTags (Transform Node)
									</dt>
									<dd>
										<p>This node uses the table tag and the cue tag to establish the relationship between various coordinate frames (camera, end-effector, table, etc.) to build a TF tree.
										</p>
									</dd>
									<dt>Computer Vision (Image Processing Node)
									</dt>
									<dd>
										<p>This node processes data from the depth camera and camera info topics to locate the cue ball (red ball) and other balls on the pool table. It integrates OpenCV and ROS2 for image processing and publishes the detected ball positions as TF frames for use in robot control and planning.
										</p>
									</dd>
									<dt>Planning & Execution (Control Node)
									</dt>
									<dd>
										<p>This node manages the robot's movements through various states, such as striking, standby, and home positions.
										</p>
									</dd>
									<dt>Pool Algorithm
									</dt>
									<dd>
										<p>This Python module uses ball coordinates to plan feasible striking positions for the robot.
										</p>
									</dd>
									<dt>World
									</dt>
									<dd>
										<p>This Python module represents the environment, including the table, Franka's platform, and the ceiling-mounted camera. It ensures the robot's path planning avoids obstacles by maintaining spatial relationships and tracking key elements like table corners, pockets, and ball positions using TF transforms.
										</p>
									</dd>
								</dl>
								<img src="images/poolinator-diagram.png" alt="Block Diagram of Interactions for Poolinator" class="left-aligned-image">
								<header>
									<p>Block Diagram of the Interactions Between Subsystems
									</p>
								</header>
								<h2>Custom ROS2 MoveIt API
								</h2>
								<p>Prior to this project, our team developed a MoveIt API with features to aid the Franka arm in planning trajectories and controlling movements:
								</p>
								<dl>
									<dt>Pose-based Planning</dt>
									<dd>
										<p>Plan trajectories to reach specific end-effector poses, such as the strike position or standby position.
										</p>
									</dd>
									<dt>Joint State Planning</dt>
									<dd>
										<p>Plan trajectories for moving the robot to predefined joint configurations.
										</p>
									</dd>
									<dt>Cartesian Path Planning</dt>
									<dd>
										<p>Plan linear motions for specific tasks, such as moving the end-effector along the x-axis during a cue strike.
										</p>
									</dd>
									<dt>Inverse Kinematics</dt>
									<dd>
										<p>Convert end-effector poses into joint configurations for precise execution of planned trajectories.
										</p>
									</dd>
									<dt>Planning Scene Setup</dt>
									<dd>
										<p>Create the planning scene to account for the table, camera, and other obstacles, which ensures collision-free trajectory execution.
										</p>
									</dd>
								</dl>
								<h2>Computer Vision (Image Processing Node)
								</h2>
								<p>I was responsible for the computer vision portion of the project, which used the Intel RealSense camera to detect the pool ball coordinates. These coordinates were fed into the pool algorithm to plan trajectories for striking balls into pockets.

									The image processing pipeline focused on two main colors: red (cue ball) and blue (other balls to be pocketed). To process each ball color, I first isolated the color using HSV-based color segmentation (Hue, Saturation, and Value), then filtered out contours with areas outside the expected ball size to reduce noise. Using the centroids of the contours, I determined the x and y coordinates of the balls. The depth information from the camera was then used to calculate the z coordinate.
									
									Pixel coordinates were converted into real-world coordinates (in meters) using the camera intrinsic values provided by the camera_info topic. These real-world coordinates were broadcasted as TF frames relative to the robot's base frame, enabling easy location lookup.
									
									</p>
								<h2>Future Work</h2>
								<p>The current computer vision approach relies on HSV values to locate the balls, which can be inconsistent under varying lighting conditions. For future work, we would consider training a YOLO model on images of pool balls to identify them more robustly. This could potentially allow us to use the full set of pool balls to play the game.

									Another stretch goal would be to use the cue provided with the game instead of a custom one.</p>
							</section>

					</div>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; An Nguyen</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>